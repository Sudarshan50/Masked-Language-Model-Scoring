<div align="center">

# ğŸ“ AIS710: BLIMP Evaluation Interface

<img src="images/hero.png" alt="BLIMP Evaluation Interface" width="100%" style="border-radius: 10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);" />

<br>

### ğŸš€ A Comprehensive Web-Based Evaluation System for Language Models

<p align="center">
  <i>Testing minimal sentence pairs for grammatical correctness and semantic plausibility</i>
</p>

<p align="center">
  <a href="#-quick-start"><img src="https://img.shields.io/badge/Quick-Start-blue?style=for-the-badge&logo=rocket" alt="Quick Start"></a>
  <a href="#-features"><img src="https://img.shields.io/badge/Features-Explore-green?style=for-the-badge&logo=star" alt="Features"></a>
  <a href="#-installation"><img src="https://img.shields.io/badge/Install-Guide-orange?style=for-the-badge&logo=download" alt="Install"></a>
  <a href="#-api-documentation"><img src="https://img.shields.io/badge/API-Docs-red?style=for-the-badge&logo=book" alt="API"></a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-3776AB?style=flat-square&logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/Flask-3.1.2-000000?style=flat-square&logo=flask&logoColor=white" alt="Flask">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=flat-square&logo=pytorch&logoColor=white" alt="PyTorch">
  <img src="https://img.shields.io/badge/Ollama-Supported-00ADD8?style=flat-square&logo=go&logoColor=white" alt="Ollama">
  <img src="https://img.shields.io/badge/HuggingFace-Transformers-FFD21E?style=flat-square&logo=huggingface&logoColor=black" alt="HuggingFace">
</p>

<br>

<table>
  <tr>
    <td align="center" width="50%">
      <img src="images/prof.png" alt="Prof. Ashwini Vaidya" width="150" style="border-radius: 50%; border: 3px solid #6366f1;" />
      <br><br>
      <h3>ğŸ‘©â€ğŸ« Course Project</h3>
      <p>
        Developed as part of <b>AIS710 Course</b><br>
        under the guidance of<br>
        <b>Prof. Ashwini Vaidya</b>
      </p>
    </td>
    <td align="center" width="50%">
      <h3>ğŸ¯ Key Highlights</h3>
      <p align="left">
        âœ… Multi-Model Comparison<br>
        âœ… Real-Time Evaluation<br>
        âœ… Interactive Visualizations<br>
        âœ… Bulk Processing Support<br>
        âœ… Export & Analysis Tools
      </p>
    </td>
  </tr>
</table>

</div>

<br>

---

## ğŸ“‹ Table of Contents

<details open>
<summary><b>Click to expand/collapse</b></summary>

- [ğŸ” Overview](#-overview)
- [âœ¨ Features](#-features)
- [ğŸš€ Installation](#-installation)
- [ğŸ¯ Quick Start](#-quick-start)
- [ğŸ—ï¸ Project Architecture](#ï¸-project-architecture)
- [ğŸ’¡ Usage](#-usage)
- [ğŸ“Š Evaluation Methodology](#-evaluation-methodology)
- [ğŸ“ Data Format](#-data-format)
- [ğŸ¤– Supported Models](#-supported-models)
- [ğŸ”Œ API Documentation](#-api-documentation)
- [ğŸ“ Examples](#-examples)
- [ğŸ› ï¸ Troubleshooting](#ï¸-troubleshooting)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)

</details>

---

## ğŸ” Overview

<div align="center">

### ğŸ¯ The BLIMP Evaluation Interface

*A powerful tool for evaluating language models on minimal pairs - sentence pairs that differ in grammaticality or semantic plausibility*

</div>

<table>
<tr>
<td width="50%">

#### ğŸŒŸ What We Offer

The system supports both **Ollama** (local LLMs) and **HuggingFace** models, providing:

- ğŸŒ **Interactive Web Interface** with real-time evaluation
- ğŸ’» **Command-Line Tools** for automation
- ğŸ“Š **Detailed Analytics** with charts
- ğŸ”„ **Dual Evaluation Modes**
- ğŸ¯ **Multi-Model Comparison**

</td>
<td width="50%">

#### ğŸ¨ Key Capabilities

- âœ… Evaluate **grammatical correctness** (syntax)
- âœ… Assess **semantic plausibility** (meaning)
- âœ… Compare **model performance** across architectures
- âœ… Visualize results with **interactive charts**
- âœ… Export results for **further analysis**

</td>
</tr>
</table>

<br>

<div align="center">

### ğŸ“Š Supported Model Types

| ğŸ¦™ Ollama Models | ğŸ¤— HuggingFace Models |
|:---:|:---:|
| DeepSeek-R1, Qwen2.5 | BERT, RoBERTa |
| Llama3, Mistral | GPT-2, DistilBERT |
| Phi4 | ALBERT |

</div>

---

## âœ¨ Features

<div align="center">

### ğŸ–¥ï¸ Dual Interface Design

</div>

<table>
<tr>
<td width="50%" valign="top">

### ğŸŒ Web Interface (`app.py`)

<img src="https://img.shields.io/badge/Port-5001-blue?style=flat-square" alt="Port"> <img src="https://img.shields.io/badge/Status-Production Ready-success?style=flat-square" alt="Status">

#### ğŸ“± Single Evaluation Mode
```
âœ“ Test individual sentence pairs in real-time
âœ“ Select multiple models simultaneously
âœ“ Interactive tooltips for metrics
âœ“ Visual comparison charts (Chart.js)
âœ“ Instant results (0-10 scale)
```

#### ğŸ“Š Bulk Evaluation Mode
```
âœ“ Upload CSV with multiple pairs
âœ“ Real-time progress tracking
âœ“ Summary statistics
âœ“ Performance analytics (bar & line charts)
âœ“ Export results as CSV
âœ“ Cancel evaluation mid-process
```

</td>
<td width="50%" valign="top">

### ğŸ’» Command-Line Tools

<img src="https://img.shields.io/badge/CLI-Available-orange?style=flat-square" alt="CLI"> <img src="https://img.shields.io/badge/Automation-Ready-green?style=flat-square" alt="Automation">

#### ğŸ¦™ Ollama Evaluation
```bash
scripts/evaluate_ollama.py
```
- Local LLM evaluation
- Token probability scoring
- JSON/CSV output formats
- Progress tracking (tqdm)

#### ğŸ¤— HuggingFace Evaluation
```bash
scripts/evaluate_blimp_hf.py
```
- MLM & CLM support
- Auto device detection (CPU/CUDA/MPS)
- Efficient batch processing
- Category-wise reporting

</td>
</tr>
</table>

<br>

<div align="center">

### ğŸ“Š Visualization & Analytics

<img src="https://img.shields.io/badge/Charts-Interactive-blueviolet?style=for-the-badge&logo=chartdotjs" alt="Charts">
<img src="https://img.shields.io/badge/Design-Responsive-ff69b4?style=for-the-badge&logo=css3" alt="Responsive">
<img src="https://img.shields.io/badge/Export-Ready-yellow?style=for-the-badge&logo=files" alt="Export">

ğŸ“ˆ **Interactive Bar Charts** â€¢ ğŸ“‰ **Line Charts** â€¢ ğŸ¨ **Gradient Styling** â€¢ ğŸ’¡ **Tooltip Explanations**

</div>

---

## ğŸš€ Installation

<div align="center">

### âš¡ Get Started in 3 Steps

<img src="https://img.shields.io/badge/Time-5 Minutes-success?style=for-the-badge&logo=clock" alt="Time">
<img src="https://img.shields.io/badge/Difficulty-Easy-green?style=for-the-badge&logo=checkmarx" alt="Difficulty">

</div>

<br>

### ğŸ“‹ Prerequisites

<table>
<tr>
<td align="center" width="25%">
  <img src="https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python"><br>
  <b>Python 3.8+</b>
</td>
<td align="center" width="25%">
  <img src="https://img.shields.io/badge/pip-Package Manager-3776AB?style=for-the-badge&logo=pypi&logoColor=white" alt="pip"><br>
  <b>pip</b>
</td>
<td align="center" width="25%">
  <img src="https://img.shields.io/badge/Ollama-Optional-00ADD8?style=for-the-badge&logo=go&logoColor=white" alt="Ollama"><br>
  <b>Ollama (Optional)</b>
</td>
<td align="center" width="25%">
  <img src="https://img.shields.io/badge/GPU-Optional-76B900?style=for-the-badge&logo=nvidia&logoColor=white" alt="GPU"><br>
  <b>GPU (Optional)</b>
</td>
</tr>
</table>

<br>

### ğŸ“¦ Step-by-Step Installation

<details open>
<summary><b>ğŸ”½ Click to expand installation steps</b></summary>

<br>

#### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Sudarshan50/Masked-Language-Model-Scoring.git
cd AIS710
```

<div align="center">
<img src="https://img.shields.io/badge/âœ“-Repository Cloned-success?style=flat-square" alt="Step 1">
</div>

<br>

#### 2ï¸âƒ£ Install Python Dependencies

```bash
pip install -r requirements.txt
```

<table>
<tr>
<td><b>ğŸ“¦ Package</b></td>
<td><b>ğŸ”¢ Version</b></td>
<td><b>ğŸ“ Purpose</b></td>
</tr>
<tr>
<td><code>transformers</code></td>
<td>â‰¥4.30.0</td>
<td>HuggingFace models</td>
</tr>
<tr>
<td><code>torch</code></td>
<td>â‰¥1.12.0</td>
<td>Neural networks</td>
</tr>
<tr>
<td><code>flask</code></td>
<td>â‰¥2.3.0</td>
<td>Web framework</td>
</tr>
<tr>
<td><code>ollama</code></td>
<td>â‰¥0.3.0</td>
<td>Ollama client</td>
</tr>
<tr>
<td><code>tqdm</code></td>
<td>latest</td>
<td>Progress bars</td>
</tr>
</table>

<div align="center">
<img src="https://img.shields.io/badge/âœ“-Dependencies Installed-success?style=flat-square" alt="Step 2">
</div>

<br>

#### 3ï¸âƒ£ Install Ollama (Optional for Local LLMs)

<table>
<tr>
<td width="33%" align="center">

**ğŸ macOS**
```bash
brew install ollama
```

</td>
<td width="33%" align="center">

**ğŸ§ Linux**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

</td>
<td width="33%" align="center">

**ğŸªŸ Windows**

Download from [ollama.com](https://ollama.com)

</td>
</tr>
</table>

<div align="center">
<img src="https://img.shields.io/badge/âœ“-Ollama Installed-success?style=flat-square" alt="Step 3">
</div>

<br>

#### 4ï¸âƒ£ Pull Ollama Models (Optional)

```bash
# ğŸš€ Recommended models for testing
ollama pull qwen2.5:3b      # Fast & efficient
ollama pull deepseek-r1:7b  # Reasoning-focused
ollama pull llama3.1:8b     # Meta's latest
ollama pull mistral:7b      # High-quality
```

<div align="center">
<img src="https://img.shields.io/badge/âœ“-Models Ready-success?style=flat-square" alt="Step 4">
<br><br>
<img src="https://img.shields.io/badge/ğŸ‰-Installation Complete!-blueviolet?style=for-the-badge" alt="Complete">
</div>

</details>

---

## ğŸ¯ Quick Start

<div align="center">

### ğŸš€ Launch in 60 Seconds

<img src="https://img.shields.io/badge/Interface-Web-blue?style=for-the-badge&logo=google-chrome" alt="Web">
<img src="https://img.shields.io/badge/CLI-Available-orange?style=for-the-badge&logo=gnome-terminal" alt="CLI">

</div>

<br>

<table>
<tr>
<td width="50%" valign="top">

### ğŸŒ Web Interface (Recommended)

<img src="https://img.shields.io/badge/Step 1-Start Ollama-00ADD8?style=flat-square&logo=go" alt="Step 1">

```bash
ollama serve
```

<img src="https://img.shields.io/badge/Step 2-Launch Flask-000000?style=flat-square&logo=flask" alt="Step 2">

```bash
python3 app.py
```

<img src="https://img.shields.io/badge/Step 3-Open Browser-FF6C37?style=flat-square&logo=google-chrome" alt="Step 3">

```
ğŸŒ http://localhost:5001
```

<img src="https://img.shields.io/badge/Step 4-Start Evaluating-success?style=flat-square&logo=checkmarx" alt="Step 4">

- **Single Mode**: Enter sentence pairs
- **Bulk Mode**: Upload CSV file
- Select models & click "Evaluate"
- View results with charts!

</td>
<td width="50%" valign="top">

### ğŸ’» Command Line Interface

<img src="https://img.shields.io/badge/Ollama-Models-00ADD8?style=flat-square&logo=go" alt="Ollama">

```bash
python scripts/evaluate_ollama.py \
  --models qwen2.5:3b deepseek-r1:7b \
  --data data/minimal_pairs.jsonl \
  --output results.csv
```

<img src="https://img.shields.io/badge/HuggingFace-Models-FFD21E?style=flat-square&logo=huggingface" alt="HuggingFace">

```bash
python scripts/evaluate_blimp_hf.py \
  --models bert-base-uncased:mlm gpt2:clm \
  --data data/minimal_pairs.jsonl \
  --output results.csv
```

<br>

> ğŸ’¡ **Tip**: Use the web interface for interactive exploration and CLI for automation!

</td>
</tr>
</table>

<br>

<div align="center">

### ğŸ¬ Demo Workflow

```mermaid
graph LR
    A[ğŸ“ Prepare Data] --> B[ğŸ”§ Select Models]
    B --> C[â–¶ï¸ Run Evaluation]
    C --> D[ğŸ“Š View Results]
    D --> E[ğŸ’¾ Export Data]
    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#e8f5e9
    style D fill:#fff3e0
    style E fill:#fce4ec
```

</div>

---

## ğŸ—ï¸ Project Architecture

```
AIS710/
â”‚
â”œâ”€â”€ app.py                              # Flask web application (395 lines)
â”‚   â”œâ”€â”€ Single evaluation endpoint
â”‚   â”œâ”€â”€ Bulk evaluation with progress tracking
â”‚   â”œâ”€â”€ Model discovery (Ollama + HuggingFace)
â”‚   â”œâ”€â”€ CSV download endpoint
â”‚   â””â”€â”€ Auto-device detection (MPS/CUDA/CPU)
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html                     # Web interface (1620 lines)
â”‚       â”œâ”€â”€ Single evaluation tab
â”‚       â”œâ”€â”€ Bulk evaluation tab
â”‚       â”œâ”€â”€ Chart.js visualizations
â”‚       â”œâ”€â”€ Tooltips with explanations
â”‚       â””â”€â”€ Responsive gradient design
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ evaluate_ollama.py             # Ollama evaluation engine (20KB)
â”‚   â”‚   â”œâ”€â”€ OllamaEvaluator class
â”‚   â”‚   â”œâ”€â”€ Token probability extraction
â”‚   â”‚   â”œâ”€â”€ Score normalization (0-10 scale)
â”‚   â”‚   â””â”€â”€ CLI interface with argparse
â”‚   â”‚
â”‚   â””â”€â”€ evaluate_blimp_hf.py           # HuggingFace evaluation (7KB)
â”‚       â”œâ”€â”€ BLIMPEvaluator integration
â”‚       â”œâ”€â”€ MLM and CLM support
â”‚       â”œâ”€â”€ Batch processing
â”‚       â””â”€â”€ Device auto-detection
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ eval_plausibility/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ blimp_evaluator.py         # Core evaluator (403 lines)
â”‚       â”‚   â”œâ”€â”€ CLM scoring (Causal LM)
â”‚       â”‚   â”œâ”€â”€ MLM scoring (Masked LM)
â”‚       â”‚   â”œâ”€â”€ Token alignment
â”‚       â”‚   â””â”€â”€ Category-wise metrics
â”‚       â”‚
â”‚       â””â”€â”€ eval.py                    # Scoring functions
â”‚           â”œâ”€â”€ score_sentence_clm()
â”‚           â”œâ”€â”€ score_sentence_mlm_pll_word_l2r()
â”‚           â””â”€â”€ Tokenization utilities
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ minimal_pairs.jsonl            # Test pairs (JSONL format)
â”‚   â”œâ”€â”€ minimal_pairs.csv              # Test pairs (CSV format)
â”‚   â”œâ”€â”€ extensive_test_pairs.jsonl     # Extended test set
â”‚   â””â”€â”€ image.png                      # Documentation assets
â”‚
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ README.md                          # This file
â””â”€â”€ WEB_INTERFACE_GUIDE.md             # Detailed web interface docs
```

### Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User Interface                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Web Browser   â”‚              â”‚  Command Line       â”‚   â”‚
â”‚  â”‚  (Port 5001)   â”‚              â”‚  (Terminal)         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                 â”‚
            â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Flask App        â”‚        â”‚  Evaluation Scripts      â”‚
â”‚      (app.py)         â”‚        â”‚  - evaluate_ollama.py    â”‚
â”‚  - REST API           â”‚        â”‚  - evaluate_blimp_hf.py  â”‚
â”‚  - Model Management   â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  - Progress Tracking  â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
            â”‚                               â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Core Evaluation Library     â”‚
        â”‚   (src/eval_plausibility/)    â”‚
        â”‚   - BLIMPEvaluator            â”‚
        â”‚   - Token scoring             â”‚
        â”‚   - Probability computation   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚
        â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama Models â”‚      â”‚ HuggingFace Models â”‚
â”‚ (Local LLMs)  â”‚      â”‚ (Transformers)     â”‚
â”‚ - Qwen        â”‚      â”‚ - BERT             â”‚
â”‚ - DeepSeek    â”‚      â”‚ - GPT-2            â”‚
â”‚ - Llama       â”‚      â”‚ - RoBERTa          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ Usage

### Web Interface

#### Single Evaluation
1. Navigate to **Single Evaluation** tab
2. Enter grammatical sentence (e.g., "I gave John the button.")
3. Enter ungrammatical sentence (e.g., "I gave John the wall.")
4. Select one or more models:
   - **Ollama Models**: qwen2.5:3b, deepseek-r1:7b, llama3.1:8b
   - **HuggingFace Models**: gpt2, bert-base-uncased, roberta-base
5. Click **Evaluate**
6. View results table with:
   - Good Score (0-10): Plausibility of grammatical sentence
   - Bad Score (0-10): Plausibility of ungrammatical sentence
   - Verdict: âœ“ (Correct) if Good Score > Bad Score
   - Time: Evaluation duration
7. Scroll to see comparison bar chart

#### Bulk Evaluation
1. Navigate to **Bulk Evaluation** tab
2. Prepare CSV file with columns:
   - `good_sentence`: Grammatical/plausible sentences
   - `bad_sentence`: Ungrammatical/implausible sentences
3. Click **Choose File** and upload CSV
4. Select models for evaluation
5. Click **Evaluate Bulk**
6. Monitor progress bar showing:
   - Current pair being processed
   - Percentage complete
   - Current model
7. View results:
   - **Detailed Results Table**: All pairs with scores and verdicts
   - **Summary Statistics**: Total pairs, overall accuracy, average time
   - **Performance Analytics**: Bar chart (accuracy) and line chart (performance trend)
8. Click **Download CSV** to export results

### Command-Line Tools

#### 1. Ollama Evaluation

**Basic Usage:**
```bash
python scripts/evaluate_ollama.py \
  --models qwen2.5:3b \
  --data data/minimal_pairs.jsonl
```

**Multiple Models:**
```bash
python scripts/evaluate_ollama.py \
  --models qwen2.5:3b deepseek-r1:7b llama3.1:8b \
  --data data/minimal_pairs.jsonl \
  --output results.csv
```

**With JSON Output:**
```bash
python scripts/evaluate_ollama.py \
  --models qwen2.5:3b \
  --data data/minimal_pairs.jsonl \
  --output results.json \
  --format json
```

#### 2. HuggingFace Evaluation

**Masked Language Model (MLM):**
```bash
python scripts/evaluate_blimp_hf.py \
  --models bert-base-uncased:mlm roberta-base:mlm \
  --data data/minimal_pairs.jsonl \
  --output results.csv
```

**Causal Language Model (CLM):**
```bash
python scripts/evaluate_blimp_hf.py \
  --models gpt2:clm \
  --data data/minimal_pairs.jsonl \
  --output results.csv
```

**Mixed Models:**
```bash
python scripts/evaluate_blimp_hf.py \
  --models bert-base-uncased:mlm gpt2:clm distilbert-base-uncased:mlm \
  --data data/minimal_pairs.jsonl \
  --device cuda \
  --output results.csv
```

---

## ğŸ“Š Evaluation Methodology

### Scoring System

#### Good Score (0-10)
Measures the **grammatical correctness** and **semantic plausibility** of the grammatical sentence:
- **10**: Perfect grammar and highly plausible
- **7-9**: Good grammar with minor issues
- **4-6**: Moderate grammaticality
- **0-3**: Poor grammar or implausible

#### Bad Score (0-10)
Measures how the model scores the ungrammatical/implausible sentence:
- Lower bad scores indicate better model discrimination
- High bad scores suggest the model accepts implausible sentences

#### Verdict
- **âœ“ Correct**: Good Score > Bad Score (model correctly identifies good sentence)
- **âœ— Incorrect**: Bad Score >= Good Score (model fails to discriminate)

### Calculation Methods

#### Ollama Models (Token Probability)
1. Generate sentence with token logprobs
2. Extract log probabilities for each token
3. Convert to linear probabilities
4. Compute average probability across tokens
5. Normalize to 0-10 scale:
   ```
   score = (avg_probability Ã— 20) - 10
   score = max(0, min(10, score))
   ```

#### HuggingFace Models

**MLM (Masked Language Models):**
- Mask each word sequentially
- Compute probability of correct token
- Aggregate using pseudo-log-likelihood (PLL)
- Normalize to 0-10 scale

**CLM (Causal Language Models):**
- Compute forward probability (left-to-right)
- Calculate log-likelihood per token
- Average across sequence
- Normalize to 0-10 scale

---

## ğŸ“ Data Format

### JSONL Format (Recommended)
```jsonl
{"good": "I gave John the button.", "bad": "I gave John the wall."}
{"good": "She ate the apple.", "bad": "She ate the computer."}
{"good": "He put the key in his pocket.", "bad": "He put the house in his pocket."}
```

### CSV Format
```csv
good_sentence,bad_sentence
I gave John the button.,I gave John the wall.
She ate the apple.,She ate the computer.
He put the key in his pocket.,He put the house in his pocket.
```

### Sample Test Cases

The `data/minimal_pairs.jsonl` includes diverse test pairs:

**Semantic Anomalies:**
- "I eat biscuit with tea" vs "I eat plate with tea"
- "I ordered a cycle" vs "I ordered a mountain"
- "She drinks water every day" vs "She drinks furniture every day"

**Size Implausibility:**
- "He has a calculator in his pocket" vs "He has a statue in his pocket"
- "She picked up a pen" vs "She picked up the sky"

**Action-Object Mismatch:**
- "She read the book" vs "She drank the book"
- "He painted the wall" vs "He painted the time"

---

## ğŸ¤– Supported Models

<div align="center">

### ğŸ¦¾ Powerful Language Models at Your Fingertips

</div>

<br>

<table>
<tr>
<td width="50%" valign="top">

### ğŸ¦™ Ollama Models (Local LLMs)

<div align="center">
<img src="https://img.shields.io/badge/Ollama-Local Deployment-00ADD8?style=for-the-badge&logo=go&logoColor=white" alt="Ollama">
</div>

<br>

| ğŸ·ï¸ Model | ğŸ“¦ Size | âš¡ Speed | ğŸ“ Description |
|:---------|:-------:|:-------:|:---------------|
| **qwen2.5:3b** | 3B | ğŸš€ğŸš€ğŸš€ | Fast, efficient Chinese-English |
| **qwen2.5:7b** | 7B | ğŸš€ğŸš€ | Balanced performance & speed |
| **deepseek-r1:7b** | 7B | ğŸš€ğŸš€ | Reasoning-focused model |
| **llama3.1:8b** | 8B | ğŸš€ğŸš€ | Meta's latest Llama |
| **mistral:7b** | 7B | ğŸš€ğŸš€ | High-quality open model |
| **phi4:latest** | 14B | ğŸš€ | Microsoft's efficient model |

<br>

**ğŸ“¥ Installation:**
```bash
ollama pull qwen2.5:3b
ollama pull deepseek-r1:7b
ollama pull llama3.1:8b
```

</td>
<td width="50%" valign="top">

### ğŸ¤— HuggingFace Models

<div align="center">
<img src="https://img.shields.io/badge/HuggingFace-Transformers-FFD21E?style=for-the-badge&logo=huggingface&logoColor=black" alt="HuggingFace">
</div>

<br>

#### ğŸ­ Masked Language Models (MLM)

| ğŸ·ï¸ Model | ğŸ“Š Params | ğŸ¯ Use Case |
|:---------|:----------|:-----------|
| **bert-base-uncased** | 110M | Original BERT base |
| **roberta-base** | 125M | Optimized BERT variant |
| **distilbert-base** | 66M | Distilled (faster) |
| **albert-base-v2** | 12M | Lightweight BERT |

#### ğŸ¯ Causal Language Models (CLM)

| ğŸ·ï¸ Model | ğŸ“Š Params | ğŸ¯ Use Case |
|:---------|:----------|:-----------|
| **gpt2** | 124M | OpenAI GPT-2 base |
| **gpt2-medium** | 355M | Larger GPT-2 |
| **gpt2-large** | 774M | Even larger GPT-2 |

<br>

> ğŸ”„ **Auto-download**: Models automatically download on first use

</td>
</tr>
</table>

<br>

<div align="center">

### ğŸ¨ Model Selection Guide

| ğŸ¯ Use Case | ğŸ’¡ Recommended Models |
|:-----------|:---------------------|
| **ğŸš€ Speed Priority** | qwen2.5:3b, distilbert-base |
| **ğŸ¯ Accuracy Priority** | llama3.1:8b, roberta-base |
| **âš–ï¸ Balanced** | qwen2.5:7b, bert-base-uncased |
| **ğŸ§  Reasoning** | deepseek-r1:7b, gpt2-medium |

</div>

---

## ğŸ”Œ API Documentation

### REST Endpoints

#### 1. Home Page
```http
GET /
```
**Response**: HTML web interface

#### 2. Get Available Models
```http
GET /api/models
```
**Response:**
```json
{
  "ollama": ["qwen2.5:3b", "deepseek-r1:7b"],
  "huggingface": ["gpt2", "bert-base-uncased", "roberta-base"]
}
```

#### 3. Single Evaluation
```http
POST /api/evaluate
Content-Type: application/json

{
  "good_sentence": "I gave John the button.",
  "bad_sentence": "I gave John the wall.",
  "models": ["qwen2.5:3b", "gpt2"]
}
```

**Response:**
```json
{
  "results": [
    {
      "model": "qwen2.5:3b",
      "good_score": 8.5,
      "bad_score": 3.2,
      "correct": true,
      "time": 1.24
    },
    {
      "model": "gpt2",
      "good_score": 7.8,
      "bad_score": 4.1,
      "correct": true,
      "time": 0.85
    }
  ]
}
```

#### 4. Bulk Evaluation
```http
POST /api/evaluate_bulk
Content-Type: multipart/form-data

file: <CSV file>
models: ["qwen2.5:3b", "gpt2"]
```

**Response:** Streaming JSON with progress updates

#### 5. Get Progress
```http
GET /api/progress
```
**Response:**
```json
{
  "current": 5,
  "total": 10,
  "status": "running",
  "current_model": "qwen2.5:3b",
  "current_pair": 5
}
```

#### 6. Cancel Evaluation
```http
POST /api/cancel
```
**Response:**
```json
{"status": "cancelled"}
```

#### 7. Download Results
```http
GET /api/download_csv
```
**Response**: CSV file download

---

## ğŸ“ Examples

### Example 1: Single Pair Evaluation

**Input:**
- Good: "The cat sat on the mat."
- Bad: "The cat sat on the sky."
- Models: qwen2.5:3b, bert-base-uncased

**Output:**
| Model | Good Score | Bad Score | Verdict | Time |
|-------|-----------|-----------|---------|------|
| qwen2.5:3b | 9.2 | 2.8 | âœ“ | 1.1s |
| bert-base-uncased | 8.7 | 3.5 | âœ“ | 0.6s |

### Example 2: Bulk Evaluation

**Input CSV (test.csv):**
```csv
good_sentence,bad_sentence
I gave John the button.,I gave John the wall.
She ate the apple.,She ate the computer.
He drinks water.,He drinks furniture.
```

**Command:**
```bash
# Via web interface: Upload test.csv, select models, click Evaluate
# Via CLI:
python scripts/evaluate_ollama.py --models qwen2.5:3b --data test.csv
```

**Output:**
- Detailed results table with 3 rows
- Accuracy: 100% (3/3 correct)
- Average time: 1.2s per pair
- Charts showing model performance

### Example 3: Multi-Model Comparison

**Command:**
```bash
python scripts/evaluate_ollama.py \
  --models qwen2.5:3b deepseek-r1:7b llama3.1:8b \
  --data data/extensive_test_pairs.jsonl \
  --output comparison.csv
```

**Result**: CSV file with side-by-side model scores for analysis

---

## ğŸ› ï¸ Troubleshooting

### Common Issues

#### 1. Ollama Connection Error
**Error:** `Connection refused to localhost:11434`

**Solution:**
```bash
# Start Ollama service
ollama serve
```

#### 2. Model Not Found
**Error:** `Model 'qwen2.5:3b' not found`

**Solution:**
```bash
# Pull the model first
ollama pull qwen2.5:3b
```

#### 3. CUDA Out of Memory
**Error:** `CUDA out of memory`

**Solution:**
```bash
# Use CPU instead
python scripts/evaluate_blimp_hf.py --device cpu --models bert-base-uncased:mlm
```

Or use smaller models:
```bash
# Use DistilBERT instead of BERT
python scripts/evaluate_blimp_hf.py --models distilbert-base-uncased:mlm
```

#### 4. Import Error
**Error:** `ModuleNotFoundError: No module named 'transformers'`

**Solution:**
```bash
pip install -r requirements.txt
```

#### 5. Port Already in Use
**Error:** `Address already in use: Port 5001`

**Solution:**
```bash
# Find and kill process using port 5001
lsof -ti:5001 | xargs kill -9

# Or change port in app.py
# app.run(debug=True, host='0.0.0.0', port=5002)
```

#### 6. Slow Evaluation
**Issue:** Models taking too long

**Solution:**
- Use smaller models (3B instead of 7B)
- Enable GPU acceleration (add CUDA support)
- Reduce batch size in evaluate_blimp_hf.py
- Use MPS on Apple Silicon:
  ```python
  # Auto-detected in app.py
  device = "mps"  # For M1/M2/M3 Macs
  ```

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

### Areas for Improvement
1. **Model Support**: Add support for new models (Claude, Gemini, etc.)
2. **Evaluation Metrics**: Implement additional scoring methods
3. **Visualization**: Enhance charts with more interactive features
4. **Performance**: Optimize batch processing and caching
5. **Testing**: Add more unit tests and integration tests
6. **Documentation**: Improve examples and tutorials

### Development Setup
```bash
# Clone repository
git clone https://github.com/Sudarshan50/Masked-Language-Model-Scoring.git
cd AIS710

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run tests
pytest tests/

# Start development server
python3 app.py
```

### Submitting Changes
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“š Additional Resources

- **WEB_INTERFACE_GUIDE.md**: Detailed web interface documentation
- **Ollama Documentation**: [ollama.com/docs](https://ollama.com/docs)
- **HuggingFace Transformers**: [huggingface.co/docs/transformers](https://huggingface.co/docs/transformers)
- **Flask Documentation**: [flask.palletsprojects.com](https://flask.palletsprojects.com)
- **Chart.js**: [chartjs.org](https://www.chartjs.org)

---

## ğŸ“„ License

<div align="center">

<br>

### ğŸ“œ Copyright & Licensing

<img src="https://img.shields.io/badge/License-Educational-blue?style=for-the-badge&logo=academia" alt="License">
<img src="https://img.shields.io/badge/Year-2025-green?style=for-the-badge" alt="Year">

<br><br>

**Â© 2025 â€¢ BLIMP Evaluation Interface**

<br>

<table>
<tr>
<td align="center" width="50%">

### ğŸ‘¨â€ğŸ’» Developer

**Sudarshan**

<a href="https://github.com/Sudarshan50/Masked-Language-Model-Scoring">
  <img src="https://img.shields.io/badge/GitHub-Repository-181717?style=for-the-badge&logo=github" alt="GitHub">
</a>

</td>
<td align="center" width="50%">

### ğŸ‘©â€ğŸ« Academic Supervisor

**Prof. Ashwini Vaidya**

Course: **AIS710**

</td>
</tr>
</table>

<br>

---

### âš–ï¸ Usage Terms

This project is developed for **educational purposes** as part of the AIS710 course.

> âš ï¸ **Note**: For commercial use, please refer to individual model licenses:
> - Ollama models: Check respective model repositories
> - HuggingFace models: See [HuggingFace Model Hub](https://huggingface.co/models)

---

<br>

### ğŸ“ Contact & Support

<table>
<tr>
<td align="center" width="33%">

### ğŸ› Report Issues

<a href="https://github.com/Sudarshan50/Masked-Language-Model-Scoring/issues">
  <img src="https://img.shields.io/badge/Issues-Report Bug-red?style=for-the-badge&logo=github" alt="Issues">
</a>

</td>
<td align="center" width="33%">

### ğŸ’¡ Feature Requests

<a href="https://github.com/Sudarshan50/Masked-Language-Model-Scoring/issues">
  <img src="https://img.shields.io/badge/Features-Request-blue?style=for-the-badge&logo=lightbulb" alt="Features">
</a>

</td>
<td align="center" width="33%">

### ğŸ“– Documentation

<a href="#-table-of-contents">
  <img src="https://img.shields.io/badge/Docs-Read More-green?style=for-the-badge&logo=readme" alt="Docs">
</a>

</td>
</tr>
</table>

<br>

---

<br>

### ğŸŒŸ Show Your Support

If you find this project helpful, please consider giving it a â­ on GitHub!

<a href="https://github.com/Sudarshan50/Masked-Language-Model-Scoring">
  <img src="https://img.shields.io/github/stars/Sudarshan50/Masked-Language-Model-Scoring?style=social" alt="GitHub Stars">
</a>

<br><br>

---

<br>

<h2>ğŸ‰ Happy Evaluating!</h2>

<img src="https://img.shields.io/badge/Made%20with-â¤ï¸-red?style=for-the-badge" alt="Made with Love">
<img src="https://img.shields.io/badge/Powered%20by-Python-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python">
<img src="https://img.shields.io/badge/Built%20with-Flask-000000?style=for-the-badge&logo=flask&logoColor=white" alt="Flask">

<br><br>

**ğŸš€ Start evaluating language models today!**

</div>
